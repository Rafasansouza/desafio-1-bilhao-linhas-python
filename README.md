# Desafio de Processamento de 1 Bilh√£o de Linhas com Python

Este projeto foi desenvolvido durante a **Aula 5 do Bootcamp de Python para dados da Jornada de Dados**, com o objetivo de aplicar t√©cnicas eficientes de processamento de dados em larga escala utilizando Python e bibliotecas de alto desempenho.

## üìå Descri√ß√£o do Projeto

O desafio consistiu em processar um arquivo massivo com **1 bilh√£o de linhas (~14GB) (na execu√ß√£o adaptamos para 1 milh√£o de linhas)** contendo registros de temperatura de esta√ß√µes meteorol√≥gicas no seguinte formato:

```
<nome da esta√ß√£o>;<medi√ß√£o>
```

Exemplo:
```
Hamburg;12.0  
Bulawayo;8.9  
Palembang;38.8  
St. Johns;15.2  
```

O objetivo foi calcular, para cada esta√ß√£o:

- Temperatura **m√≠nima**
- Temperatura **m√©dia** (com 1 casa decimal)
- Temperatura **m√°xima**

Al√©m disso, o projeto compara o desempenho de diferentes bibliotecas Python utilizadas para a mesma tarefa, focando na efici√™ncia e no tempo de execu√ß√£o.

## üìö Aprendizados

Durante o desenvolvimento, pude explorar diversos conceitos fundamentais para quem trabalha com grandes volumes de dados:

- Leitura eficiente de arquivos massivos com Python
- C√°lculo de estat√≠sticas agregadas
- Avalia√ß√£o de performance entre diferentes bibliotecas
- Otimiza√ß√£o com bibliotecas de consulta orientadas a colunas
- An√°lise e visualiza√ß√£o de benchmarks

## ‚öôÔ∏è Tecnologias Utilizadas

- [Python 3.12.1](https://docs.python.org/release/3.12.1/)
- [Polars 0.20.3](https://pola.rs/)
- [DuckDB 0.10.0](https://duckdb.org/)
- [Dask 2024.2.0](https://www.dask.org/)
- [Pandas](https://pandas.pydata.org/)
- [Poetry](https://python-poetry.org/) (gerenciador de depend√™ncias)
- Bash + awk (como benchmark adicional)

## üöÄ Resultados

Houve uma adapta√ß√£o ao longo do projeto, para andar mais rapido alteramos o arquivo "create..." na pasta "src" para criar apenas um arquivo de 1M de linhas.
O **DuckDB** apresentou o melhor desempenho, gra√ßas √† sua arquitetura otimizada para leitura de dados em colunas e execu√ß√£o vetorizada.

## üõ†Ô∏è Como Executar

### 1. Clone este reposit√≥rio
```bash
git clone https://github.com/Rafasansouza/desafio-1-bilhao-linhas-python.git
cd desafio-1-bilhao-linhas-python
```

### 2. Configure o ambiente
```bash
pyenv local 3.12.1
poetry env use 3.12.1
poetry install --no-root
poetry lock --no-update
```

### 3. Gere o arquivo de dados
```bash
python src/create_measurements.py
```

*Observa√ß√£o: A gera√ß√£o do arquivo leva cerca de 10 minutos e o arquivo final tem aproximadamente 14GB.*

### 4. Execute os scripts
```bash
python src/using_python.py
python src/using_pandas.py
python src/using_dask.py
python src/using_polars.py
python src/using_duckdb.py
```

## üß™ Benchmark Extra com Bash + awk

Voc√™ tamb√©m pode testar a vers√£o em Bash com:

```bash
chmod +x src/using_bash_and_awk.sh
./src/using_bash_and_awk.sh 1000
```

Certifique-se de ter as ferramentas `awk`, `sort`, e `pv` (Pipe Viewer) instaladas.

### Para instalar `pv`:
- No Ubuntu/Debian:
```bash
sudo apt-get update
sudo apt-get install pv
```

- No macOS (com Homebrew):
```bash
brew install pv
```

## üéØ Conclus√£o

Este projeto foi uma √≥tima oportunidade para colocar em pr√°tica tudo que venho aprendendo sobre **processamento de dados em larga escala com Python**. Descobri que com as bibliotecas certas ‚Äî como **Polars** e **DuckDB** ‚Äî √© poss√≠vel lidar com datasets gigantescos de forma muito mais r√°pida e eficiente, mesmo em m√°quinas com poucos recursos.

Al√©m de aprofundar o uso de bibliotecas como Pandas, Dask e DuckDB, o projeto me ensinou a import√¢ncia de benchmarking e de escolher a tecnologia certa para o desafio proposto.

## üîó Reposit√≥rio de Refer√™ncia

Este projeto foi inspirado e adaptado a partir do bootcamp de python para dados da **Jornada de dados** [lvgalvao/One-Billion-Row-Challenge-Python](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python).
